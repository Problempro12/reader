"""–ú–æ–¥—É–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–Ω–µ—à–Ω–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –∫–Ω–∏–≥ (—Ç–æ–ª—å–∫–æ –§–ª–∏–±—É—Å—Ç–∞)"""

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import logging
from typing import Dict, List, Optional, Any
from bs4 import BeautifulSoup
import time
import random
import chardet
import zipfile
import io
import html
from urllib.parse import urljoin, urlparse
from .external_config import (
    FLIBUSTA_CONFIG,
    EXTERNAL_SOURCES_CONFIG,
    SECURITY_CONFIG,
    TOR_PROXY_CONFIG
)
from .cover_sources import get_book_cover_url
from requests import Session
import json
import xml.etree.ElementTree as ET
from datetime import datetime


class FlibustaTorClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –§–ª–∏–±—É—Å—Ç–æ–π —á–µ—Ä–µ–∑ Tor"""
    
    def __init__(self, use_tor: bool = True):
        self.session = Session()
        self.use_tor = use_tor
        self.logger = logging.getLogger(__name__)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ onion-–∞–¥—Ä–µ—Å —á–µ—Ä–µ–∑ Tor SOCKS –ø—Ä–æ–∫—Å–∏
        if use_tor:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ SOCKS –ø—Ä–æ–∫—Å–∏ –¥–ª—è onion-–∞–¥—Ä–µ—Å–æ–≤
            if TOR_PROXY_CONFIG:
                self.session.proxies.update(TOR_PROXY_CONFIG)
                self.logger.info(f"–ù–∞—Å—Ç—Ä–æ–µ–Ω SOCKS –ø—Ä–æ–∫—Å–∏: {TOR_PROXY_CONFIG}")
            
            self.base_url = FLIBUSTA_CONFIG['onion_url']
            self.logger.info(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ onion-–∞–¥—Ä–µ—Å—É —á–µ—Ä–µ–∑ Tor SOCKS –ø—Ä–æ–∫—Å–∏: {self.base_url}")
        else:
            raise ValueError("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Flibusta –≤–æ–∑–º–æ–∂–Ω–æ —Ç–æ–ª—å–∫–æ —á–µ—Ä–µ–∑ Tor!")
        
        self.opds_url = f"{self.base_url}/opds"
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ retry —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        retry_strategy = Retry(
            total=FLIBUSTA_CONFIG['max_retries'],
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        self.session.headers.update({
            'User-Agent': EXTERNAL_SOURCES_CONFIG['user_agent'],
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–∞–π–º–∞—É—Ç–æ–≤
        self.timeout = FLIBUSTA_CONFIG['timeout']
    
    def search_books(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –∫–Ω–∏–≥ –Ω–∞ –§–ª–∏–±—É—Å—Ç–µ —á–µ—Ä–µ–∑ Tor"""
        books = []
        
        try:
            response = self.session.get(category_url, timeout=self.timeout)
            response.raise_for_status()
            self.logger.debug(f"–ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç {category_url}, —Å—Ç–∞—Ç—É—Å: {response.status_code}, —Ä–∞–∑–º–µ—Ä: {len(response.content)} –±–∞–π—Ç")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö
            encoding = response.encoding
            if not encoding or encoding == 'ISO-8859-1':
                encoding = chardet.detect(response.content)['encoding']
                self.logger.debug(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∞: {encoding}")

            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
            content = response.content.decode(encoding or 'utf-8', errors='ignore')
            soup = BeautifulSoup(content, 'html.parser')
            self.logger.debug(f"HTML —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω BeautifulSoup.")

            books = []
            # –õ–æ–≥–∏—Ä—É–µ–º, —á—Ç–æ –º—ã –∏—â–µ–º –∫–Ω–∏–≥–∏
            self.logger.debug("–ò—â–µ–º –∫–Ω–∏–≥–∏ –≤ HTML...")

            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã 'div' —Å –∫–ª–∞—Å—Å–æ–º 'book'
            book_elements = soup.find_all('div', class_='book')
            self.logger.debug(f"–ù–∞–π–¥–µ–Ω–æ {len(book_elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ 'div' —Å –∫–ª–∞—Å—Å–æ–º 'book'.")

            for i, book_elem in enumerate(book_elements):
                if len(books) >= limit:
                    self.logger.debug(f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –∫–Ω–∏–≥ ({limit}), –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥.")
                    break

                self.logger.debug(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç –∫–Ω–∏–≥–∏ {i+1}/{len(book_elements)}...")
                book_data = self._parse_book_entry_bs(book_elem)
                if book_data:
                    books.append(book_data)
                    self.logger.debug(f"–ö–Ω–∏–≥–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∞: {book_data.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')} - {book_data.get('author', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä')}")
                else:
                    self.logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —ç–ª–µ–º–µ–Ω—Ç –∫–Ω–∏–≥–∏ {i+1}.")

            self.logger.info(f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ. –ù–∞–π–¥–µ–Ω–æ {len(books)} –∫–Ω–∏–≥ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {category_url}.")
            return books

        except requests.exceptions.RequestException as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∫–Ω–∏–≥ –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {category_url}: {e}")
            return []
        except Exception as e:
            self.logger.error(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∫–Ω–∏–≥ –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {category_url}: {e}")
            return []
            # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ OPDS —Ç–æ–ª—å–∫–æ —á–µ—Ä–µ–∑ Tor
            books = self._search_via_opds(query, limit)
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –Ω–∞ –§–ª–∏–±—É—Å—Ç–µ —á–µ—Ä–µ–∑ Tor: {e}")
        
        return books
    
    def browse_categories(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π/–∂–∞–Ω—Ä–æ–≤ —á–µ—Ä–µ–∑ OPDS"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º OPDS –∫–∞—Ç–∞–ª–æ–≥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∂–∞–Ω—Ä–æ–≤
            opds_url = f"{self.base_url}/opds/genres"
            self.logger.info(f"–ó–∞–ø—Ä–æ—Å –∂–∞–Ω—Ä–æ–≤ —á–µ—Ä–µ–∑ OPDS: {opds_url}")
            
            response = self.session.get(opds_url, timeout=self.timeout)
            response.raise_for_status()
            
            # –ü–∞—Ä—Å–∏–º XML –æ—Ç–≤–µ—Ç
            root = ET.fromstring(response.content)
            ns = {'atom': 'http://www.w3.org/2005/Atom'}
            
            categories = []
            entries = root.findall('.//atom:entry', ns)
            
            for entry in entries:
                title_elem = entry.find('atom:title', ns)
                if title_elem is not None:
                    category_name = title_elem.text
                    
                    # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é
                    links = entry.findall('atom:link', ns)
                    category_url = None
                    for link in links:

                        href = link.get('href')
                        if href:
                            category_url = f"{self.base_url}{href}"
                            break
                    
                    if category_name and category_url:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ URL (–µ—Å–ª–∏ –µ—Å—Ç—å)
                        import re
                        category_id = category_name.lower().replace(' ', '_')
                        
                        categories.append({
                            'id': category_id,
                            'name': category_name,
                            'url': category_url
                        })
            
            self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(categories)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π —á–µ—Ä–µ–∑ OPDS")
            return categories
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π —á–µ—Ä–µ–∑ OPDS: {e}")
            return []
    
    def get_all_genres(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∂–∞–Ω—Ä–æ–≤ —Å HTML-—Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å–æ –≤—Å–µ–º–∏ –∂–∞–Ω—Ä–∞–º–∏
            genres_url = f"{self.base_url}/g"
            self.logger.info(f"–ó–∞–ø—Ä–æ—Å –≤—Å–µ—Ö –∂–∞–Ω—Ä–æ–≤: {genres_url}")
            
            response = self.session.get(genres_url, timeout=self.timeout)
            response.raise_for_status()
            
            # –ü–∞—Ä—Å–∏–º HTML –æ—Ç–≤–µ—Ç
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∂–∞–Ω—Ä—ã (—Å–æ–¥–µ—Ä–∂–∞—Ç /g/ –≤ href)
            genre_links = soup.find_all('a', href=True)
            genres = []
            
            for link in genre_links:
                href = link.get('href', '')
                if '/g/' in href and href != '/g':
                    genre_name = link.text.strip()
                    if genre_name:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –Ω–µ –ø—É—Å—Ç–æ–µ
                        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                        if href.startswith('/'):
                            genre_url = f"{self.base_url}{href}"
                        else:

                            genre_url = href
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∂–∞–Ω—Ä–∞ –∏–∑ URL
                        genre_id = href.split('/')[-1] if '/' in href else href
                        
                        genres.append({
                            'id': genre_id,
                            'name': genre_name,
                            'url': genre_url
                        })
            
            self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(genres)} –∂–∞–Ω—Ä–æ–≤ —á–µ—Ä–µ–∑ HTML-–ø–∞—Ä—Å–∏–Ω–≥")
            return genres
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö –∂–∞–Ω—Ä–æ–≤: {e}")
            return []
    
    def _clean_xml_content(self, content: bytes) -> bytes:
        """–û—á–∏—Å—Ç–∫–∞ XML –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –æ—Ç HTML entities"""
        try:
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É
            text = content.decode('utf-8', errors='ignore')
            # –ó–∞–º–µ–Ω—è–µ–º HTML entities
            text = text.replace('&nbsp;', ' ')
            text = text.replace('&mdash;', '‚Äî')
            text = text.replace('&ndash;', '‚Äì')
            text = text.replace('&laquo;', '¬´')
            text = text.replace('&raquo;', '¬ª')
            text = text.replace('&hellip;', '‚Ä¶')
            # –ö–æ–¥–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ –±–∞–π—Ç—ã
            return text.encode('utf-8')
        except Exception:
            return content
    
    def browse_books_by_category(self, category_url: str, sort_by_popularity: bool = True, limit: int = 50) -> List[Dict[str, Any]]:
        self.logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category_url} (–ª–∏–º–∏—Ç: {limit})")
        self.logger.debug(f"–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏: {sort_by_popularity}")
        """–ü–æ–ª—É—á–∏—Ç—å –∫–Ω–∏–≥–∏ –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–æ–π –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏"""
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º URL –≤ OPDS —Ñ–æ—Ä–º–∞—Ç –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            opds_url = category_url
            if '/g/' in category_url and '/opds' not in category_url:
                # –ó–∞–º–µ–Ω—è–µ–º /g/ –Ω–∞ /opds/g/ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è OPDS-—Ñ–æ—Ä–º–∞—Ç–∞
                opds_url = category_url.replace('/g/', '/opds/g/')
            
            self.logger.info(f"–ó–∞–ø—Ä–æ—Å –∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category_url}")
            self.logger.info(f"OPDS URL: {opds_url}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è OPDS-—Ñ–æ—Ä–º–∞—Ç–∞
            headers = {
                'Accept': 'application/atom+xml, application/xml, text/xml, */*',
                'User-Agent': 'OPDS-Client/1.0'
            }
            response = self.session.get(opds_url, headers=headers, timeout=self.timeout)
            response.raise_for_status()
            
            self.logger.info(f"–ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç, —Ä–∞–∑–º–µ—Ä: {len(response.content)} –±–∞–π—Ç")
            self.logger.info(f"Content-Type: {response.headers.get('content-type', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º BeautifulSoup –¥–ª—è –±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò—â–µ–º –≤—Å–µ entry —ç–ª–µ–º–µ–Ω—Ç—ã
            entries = soup.find_all('entry')
            self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(entries)} –∑–∞–ø–∏—Å–µ–π entry")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ–≥–æ–≤ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            all_tags = [tag.name for tag in soup.find_all() if tag.name]
            unique_tags = list(set(all_tags))
            self.logger.info(f"–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–µ–≥–∏: {unique_tags[:10]}...")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –æ—Ç–≤–µ—Ç –∫–Ω–∏–≥–∏ –∏–ª–∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            if entries:
                first_entry = entries[0]
                self.logger.info(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—É—é –∑–∞–ø–∏—Å—å...")
                # –ï—Å–ª–∏ –µ—Å—Ç—å –∞–≤—Ç–æ—Ä, –∑–Ω–∞—á–∏—Ç —ç—Ç–æ –∫–Ω–∏–≥–∏
                author_elem = first_entry.find('author')
                self.logger.info(f"–ù–∞–π–¥–µ–Ω —ç–ª–µ–º–µ–Ω—Ç author: {author_elem is not None}")
                if author_elem:
                    name_elem = author_elem.find('name')
                    self.logger.info(f"–ù–∞–π–¥–µ–Ω —ç–ª–µ–º–µ–Ω—Ç name –≤ author: {name_elem is not None}")
                    if name_elem:
                        self.logger.info(f"–≠—Ç–æ –∫–Ω–∏–≥–∏, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –ø–∞—Ä—Å–∏–Ω–≥—É –∫–∞—Ç–∞–ª–æ–≥–∞")
                        # –≠—Ç–æ –∫–Ω–∏–≥–∏, –ø–∞—Ä—Å–∏–º –∏—Ö
                        return self._get_books_from_catalog_bs(category_url, limit)
                
                self.logger.info(f"–≠—Ç–æ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –∏—â–µ–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é")
                # –≠—Ç–æ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –≤—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é
                for entry in entries:
                    links = entry.find_all('link')
                    for link in links:
                        href = link.get('href')
                        if href and not href.startswith('http'):
                            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                            if href.startswith('/'):
                                full_href = f"{self.base_url}{href}"
                            else:
                                full_href = urljoin(opds_url, href)
                            
                            self.logger.info(f"–ü–µ—Ä–µ—Ö–æ–¥–∏–º –≤ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—é: {full_href}")
                            # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –≤—ã–∑—ã–≤–∞–µ–º —Å–µ–±—è –¥–ª—è –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                            subcategory_books = self.browse_books_by_category(full_href, sort_by_popularity, limit)
                            if subcategory_books:
                                return subcategory_books
                            break
                    if href:
                                subcategory_url = href if href.startswith('http') else f"{self.base_url}{href}"
                                # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø–æ–ª—É—á–∞–µ–º –∫–Ω–∏–≥–∏ –∏–∑ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                                books = self._get_books_from_catalog_bs(subcategory_url, limit)
                                if books:
                                    return books
            
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
            return books
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–Ω–∏–≥ –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {e}")
            return []
    
    def _get_books_from_catalog_bs(self, catalog_url: str, limit: int) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –∫–Ω–∏–≥–∏ –∏–∑ –∫–∞—Ç–∞–ª–æ–≥–∞ –∏—Å–ø–æ–ª—å–∑—É—è BeautifulSoup"""
        try:
            response = self.session.get(catalog_url, timeout=self.timeout)
            response.raise_for_status()
            
            self.logger.info(f"–ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –∏–∑ –∫–∞—Ç–∞–ª–æ–≥–∞, —Ä–∞–∑–º–µ—Ä: {len(response.content)} –±–∞–π—Ç")
            self.logger.info(f"Content-Type: {response.headers.get('content-type', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
            
            soup = BeautifulSoup(response.content, 'html.parser')
            books = []
            
            entries = soup.find_all('entry')
            self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(entries)} –∑–∞–ø–∏—Å–µ–π entry –≤ –∫–∞—Ç–∞–ª–æ–≥–µ")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ–≥–æ–≤ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            all_tags = [tag.name for tag in soup.find_all() if tag.name]
            unique_tags = list(set(all_tags))
            self.logger.info(f"–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–µ–≥–∏ –≤ –∫–∞—Ç–∞–ª–æ–≥–µ: {unique_tags[:15]}...")
            
            for i, entry in enumerate(entries[:limit]):
                self.logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø–∏—Å—å {i+1}")
                book_data = self._parse_book_entry_bs(entry)
                if book_data:
                    self.logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –∫–Ω–∏–≥–∞: {book_data.get('title', '–±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}")
                    books.append(book_data)
                else:
                    self.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–ø–∏—Å—å {i+1}")
            
            self.logger.info(f"üìä –ò—Ç–æ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–Ω–∏–≥: {len(books)}")
            return books
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–Ω–∏–≥ –∏–∑ –∫–∞—Ç–∞–ª–æ–≥–∞ {catalog_url}: {e}")
            self.logger.debug(f"–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –æ—à–∏–±–∫–∏:", exc_info=True)
            return []
    
    def _get_books_from_catalog(self, catalog_url: str, limit: int) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –∫–Ω–∏–≥–∏ –∏–∑ –∫–∞—Ç–∞–ª–æ–≥–∞"""
        try:
            response = self.session.get(catalog_url, timeout=self.timeout)
            response.raise_for_status()
            
            # –û—á–∏—â–∞–µ–º XML –æ—Ç HTML entities
            cleaned_content = self._clean_xml_content(response.content)
            root = ET.fromstring(cleaned_content)
            books = []
            
            # Namespace –¥–ª—è OPDS
            ns = {
                'atom': 'http://www.w3.org/2005/Atom',
                'opds': 'http://opds-spec.org/2010/catalog'
            }
            
            entries = root.findall('.//atom:entry', ns)
            
            for entry in entries[:limit]:
                book_data = self._parse_book_entry(entry, ns)
                if book_data:
                    books.append(book_data)
            
            return books
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–Ω–∏–≥ –∏–∑ –∫–∞—Ç–∞–ª–æ–≥–∞ {catalog_url}: {e}")
            return []
    
    def _get_categories_from_catalog(self, catalog_url: str, limit: int) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏/–∂–∞–Ω—Ä—ã –∏–∑ –∫–∞—Ç–∞–ª–æ–≥–∞ (–±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ —Å—Å—ã–ª–∫–∞–º —Å–∫–∞—á–∏–≤–∞–Ω–∏—è)"""
        try:
            response = self.session.get(catalog_url, timeout=self.timeout)
            response.raise_for_status()
            
            root = ET.fromstring(response.content)
            categories = []
            
            # Namespace –¥–ª—è OPDS
            ns = {
                'atom': 'http://www.w3.org/2005/Atom',
                'opds': 'http://opds-spec.org/2010/catalog'
            }
            
            entries = root.findall('.//atom:entry', ns)
            
            for entry in entries[:limit]:
                # –ü–∞—Ä—Å–∏–º –∫–∞–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏—é (–±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ —Å—Å—ã–ª–∫–∞–º —Å–∫–∞—á–∏–≤–∞–Ω–∏—è)
                category_data = self._parse_category_entry(entry, ns)
                if category_data:
                    categories.append(category_data)
            
            return categories
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ –∫–∞—Ç–∞–ª–æ–≥–∞ {catalog_url}: {e}")
            return []
    
    def _parse_category_entry(self, entry, ns: Dict[str, str]) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —ç–ª–µ–º–µ–Ω—Ç–∞ –∫–∞—Ç–∞–ª–æ–≥–∞ –∫–∞–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏/–∂–∞–Ω—Ä–∞"""
        try:
            title_elem = entry.find('atom:title', ns)
            title = title_elem.text if title_elem is not None else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            category_url = None
            links = entry.findall('atom:link', ns)
            
            for link in links:
                rel = link.get('rel', '')
                href = link.get('href', '')
                
                # –ò—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –ø–æ–¥–∫–∞—Ç–∞–ª–æ–≥ –∏–ª–∏ –Ω–∞–≤–∏–≥–∞—Ü–∏—é
                if rel in ['subsection', 'alternate'] or 'type' in link.attrib:
                    if href:
                        category_url = urljoin(self.base_url, href)
                        break
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é —Å—Å—ã–ª–∫—É, –±–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é
            if not category_url and links:
                href = links[0].get('href', '')
                if href:
                    category_url = urljoin(self.base_url, href)
            
            if not category_url:
                return None
            
            return {
                'title': title,
                'url': category_url,
                'download_url': category_url  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            }
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —ç–ª–µ–º–µ–Ω—Ç–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {e}")
            return None
    
    def _search_via_opds(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ OPDS –∫–∞—Ç–∞–ª–æ–≥"""
        self.logger.debug(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ OPDS –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query}' (–ª–∏–º–∏—Ç: {limit})")
        
        search_url = f"{self.opds_url}/search?searchTerm={query}"
        self.logger.debug(f"üì° –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫: {search_url}")
        
        response = self.session.get(search_url, timeout=self.timeout)
        response.raise_for_status()
        self.logger.debug(f"‚úÖ –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç, —Ä–∞–∑–º–µ—Ä: {len(response.content)} –±–∞–π—Ç")
        
        # –ü–∞—Ä—Å–∏–º OPDS XML
        root = ET.fromstring(response.content)
        books = []
        
        # Namespace –¥–ª—è OPDS
        ns = {
            'atom': 'http://www.w3.org/2005/Atom',
            'opds': 'http://opds-spec.org/2010/catalog'
        }
        
        entries = root.findall('.//atom:entry', ns)
        self.logger.debug(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(entries)} –∑–∞–ø–∏—Å–µ–π –≤ –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–º –æ—Ç–≤–µ—Ç–µ")
        
        # –ò—â–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏ –≤ –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–º –æ—Ç–≤–µ—Ç–µ
        search_links = []
        for entry in entries:
            title_elem = entry.find('atom:title', ns)
            if title_elem is not None and title_elem.text:
                title_text = title_elem.text.strip()
                
                # –ò—â–µ–º —Ç–æ–ª—å–∫–æ —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø–æ–∏—Å–∫ –∫–Ω–∏–≥ (–Ω–µ –∞–≤—Ç–æ—Ä–æ–≤)
                if any(search_term in title_text for search_term in ['–ü–æ–∏—Å–∫ –∫–Ω–∏–≥', 'Search books']):
                    for link in entry.findall('atom:link', ns):
                        href = link.get('href')
                        link_type = link.get('type')
                        if href and link_type == 'application/atom+xml;profile=opds-catalog':
                            search_links.append((title_text, href))
                            self.logger.debug(f"üîó –ù–∞–π–¥–µ–Ω–∞ –ø–æ–∏—Å–∫–æ–≤–∞—è —Å—Å—ã–ª–∫–∞: {title_text} -> {href}")
                            break
        
        self.logger.debug(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(search_links)} –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫")
        
        # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã –ø–æ–∏—Å–∫–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –ø–æ –Ω–∏–º
        if search_links:
            self.logger.debug(f"üöÄ –ü–µ—Ä–µ—Ö–æ–¥–∏–º –ø–æ –ø–æ–∏—Å–∫–æ–≤—ã–º —Å—Å—ã–ª–∫–∞–º...")
            for search_type, search_href in search_links:
                if len(books) >= limit:
                    self.logger.debug(f"‚èπÔ∏è –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –∫–Ω–∏–≥ ({limit}), –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –ø–æ–∏—Å–∫")

                    break
                    
                try:
                    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å –∑–∞–ø—Ä–æ—Å–æ–º
                    if search_href.startswith('/'):
                        full_url = f"{self.base_url}{search_href}"
                    else:
                        full_url = urljoin(self.base_url, search_href)
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä –ø–æ–∏—Å–∫–∞ –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
                    if '?' not in full_url:
                        full_url += f"?searchTerm={query}"
                    elif 'searchTerm=' not in full_url:
                        full_url += f"&searchTerm={query}"
                    
                    self.logger.debug(f"üîó –ü–µ—Ä–µ—Ö–æ–¥–∏–º –ø–æ —Å—Å—ã–ª–∫–µ '{search_type}': {full_url}")
                    
                    # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
                    search_response = self.session.get(full_url, timeout=self.timeout)
                    search_response.raise_for_status()
                    self.logger.debug(f"‚úÖ –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç –ø–æ–∏—Å–∫–æ–≤–æ–π —Å—Å—ã–ª–∫–∏, —Ä–∞–∑–º–µ—Ä: {len(search_response.content)} –±–∞–π—Ç")
                    
                    # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    search_root = ET.fromstring(search_response.content)
                    search_entries = search_root.findall('.//atom:entry', ns)
                    self.logger.debug(f"üìö –ù–∞–π–¥–µ–Ω–æ {len(search_entries)} –∫–Ω–∏–≥ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞")
                    
                    for i, entry in enumerate(search_entries):
                        if len(books) >= limit:
                            self.logger.debug(f"‚èπÔ∏è –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –∫–Ω–∏–≥ ({limit}), –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥")

                            break
                            
                        self.logger.debug(f"üìñ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–Ω–∏–≥—É {i+1}/{len(search_entries)}...")
                        book_data = self._parse_book_entry(entry, ns)
                        if book_data:
                            books.append(book_data)
                            self.logger.debug(f"‚úÖ –ö–Ω–∏–≥–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∞: {book_data.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')} - {book_data.get('author', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä')}")
                        else:
                            pass

                except Exception as e:
                    self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ—Ö–æ–¥–µ –ø–æ —Å—Å—ã–ª–∫–µ '{search_type}': {e}")

                    continue
        else:
            # –ï—Å–ª–∏ —ç—Ç–æ —É–∂–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞, –ø–∞—Ä—Å–∏–º –∏—Ö –Ω–∞–ø—Ä—è–º—É—é
            for entry in entries[:limit]:
                book_data = self._parse_book_entry(entry, ns)
                if book_data:
                    books.append(book_data)
        
        return books
    
    def _parse_book_entry_bs(self, entry) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏ –∫–Ω–∏–≥–∏ –∏–∑ OPDS –∏—Å–ø–æ–ª—å–∑—É—è BeautifulSoup"""
        try:
            title_elem = entry.find('title')
            author_elem = entry.find('author')
            summary_elem = entry.find('summary')
            
            self.logger.debug(f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–ø–∏—Å–∏:")
            self.logger.debug(f"üìñ –ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–∞–ø–∏—Å–∏ –∫–Ω–∏–≥–∏")
            self.logger.debug(f"  üìå –ó–∞–≥–æ–ª–æ–≤–æ–∫: {title_elem.get_text().strip() if title_elem else '–ù–ï–¢'}")
            self.logger.debug(f"  ‚úçÔ∏è –ê–≤—Ç–æ—Ä: {author_elem is not None}")
            self.logger.debug(f"  üìù –û–ø–∏—Å–∞–Ω–∏–µ: {summary_elem is not None}")
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∑–∞–ø–∏—Å–∏
            if title_elem:
                title_text = title_elem.get_text().strip()
                self.logger.debug(f"  üìñ –¢–µ–∫—Å—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞: {title_text}")
                if any(skip_word in title_text for skip_word in ['–ü–æ–∏—Å–∫', 'Search', '–ö–∞—Ç–∞–ª–æ–≥', 'Catalog']):
                    self.logger.debug(f"  ‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—É—é –∑–∞–ø–∏—Å—å")

                    return None
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å—Å—ã–ª–∫–∏ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è (acquisition) - —ç—Ç–æ –æ—Ç–ª–∏—á–∞–µ—Ç –∫–Ω–∏–≥–∏ –æ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            has_download_links = False
            links = entry.find_all('link')
            self.logger.debug(f"  üîó –ù–∞–π–¥–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫ –≤ –∑–∞–ø–∏—Å–∏")
            for link in links:
                rel = link.get('rel')
                href = link.get('href')
                type_attr = link.get('type')

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º rel - –º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π, —Å–ø–∏—Å–∫–æ–º –∏–ª–∏ AttributeValueList
                if rel:
                    if isinstance(rel, list):
                        rel_values = [r.strip() for r in rel]
                    elif hasattr(rel, '__iter__') and not isinstance(rel, str):
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º AttributeValueList –∏ –¥—Ä—É–≥–∏–µ –∏—Ç–µ—Ä–∏—Ä—É–µ–º—ã–µ –æ–±—ä–µ–∫—Ç—ã
                        rel_values = [str(r).strip() for r in rel]
                    else:
                        rel_values = [str(rel).strip()]
                    
                    for rel_val in rel_values:
                        if rel_val in ['http://opds-spec.org/acquisition', 'http://opds-spec.org/acquisition/open-access']:
                            has_download_links = True
                            break
                    if has_download_links:
                        break
            
            self.logger.debug(f"  üì• –°—Å—ã–ª–∫–∏ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {'–µ—Å—Ç—å' if has_download_links else '–Ω–µ—Ç'}")
            # –ï—Å–ª–∏ –Ω–µ—Ç —Å—Å—ã–ª–æ–∫ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è, —ç—Ç–æ —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è, –∞ –Ω–µ –∫–Ω–∏–≥–∞
            if not has_download_links:
                self.logger.debug(f"  ‚è≠Ô∏è –ù–µ—Ç —Å—Å—ã–ª–æ–∫ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–ø–∏—Å—å")

                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            title = title_elem.get_text().strip() if title_elem else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'
            author = ''
            if author_elem:
                name_elem = author_elem.find('name')
                if name_elem:
                    author = name_elem.get_text().strip()
            
            description = summary_elem.get_text().strip() if summary_elem else ''
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
            download_links = {}
            for link in entry.find_all('link'):
                rel = link.get('rel')
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º rel - –º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π –∏–ª–∏ —Å–ø–∏—Å–∫–æ–º
                if rel:
                    if isinstance(rel, list):
                        rel_values = [r.strip() for r in rel]
                    else:
                        rel_values = [rel.strip()]
                    
                    for rel_val in rel_values:
                        if rel_val in ['http://opds-spec.org/acquisition', 'http://opds-spec.org/acquisition/open-access']:
                            href = link.get('href')
                            mime_type = link.get('type', '')
                            
                            if href:
                                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                                if href.startswith('/'):
                                    full_url = f"{self.base_url}{href}"
                                else:
                                    full_url = href
                                
                                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞
                                file_format = self._extract_format_from_type(mime_type)
                                
                                download_links[file_format] = {
                                    'url': full_url,
                                    'type': mime_type
                                }
                            break
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∫–Ω–∏–≥–∏ –∏–∑ —Å—Å—ã–ª–æ–∫ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ–±–ª–æ–∂–∫–∏
            book_id = self._extract_book_id_from_links([{'url': link['url']} for link in download_links.values()])
            
            # –ü–æ–ª—É—á–∞–µ–º –æ–±–ª–æ–∂–∫—É –∏–∑ –≤–Ω–µ—à–Ω–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –ø–µ—Ä–µ–¥–∞–≤–∞—è ID –∫–Ω–∏–≥–∏
            cover_url = get_book_cover_url(title, author, book_id)
            
            return {
                'title': title,
                'author': author,
                'description': description,
                'download_links': download_links,
                'source': 'flibusta',
                'source_id': book_id,
                'cover_url': cover_url
            }
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–∞–ø–∏—Å–∏ OPDS: {e}")
            return None
    
    def _parse_book_entry(self, entry, ns: Dict[str, str]) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏ –∫–Ω–∏–≥–∏ –∏–∑ OPDS"""
        try:
            title = entry.find('atom:title', ns)
            author = entry.find('atom:author/atom:name', ns)
            summary = entry.find('atom:summary', ns)
            updated = entry.find('atom:updated', ns)
            
            title_text = title.text.strip() if title is not None and title.text else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
            author_text = author.text.strip() if author is not None and author.text else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä"
            
            self.logger.debug(f"  üîç –ü–∞—Ä—Å–∏–º –∑–∞–ø–∏—Å—å: '{title_text}' - {author_text}")
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∑–∞–ø–∏—Å–∏
            if title is not None:
                self.logger.debug(f"  ‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—É—é –∑–∞–ø–∏—Å—å: {title_text}")
                if any(skip_word in title_text for skip_word in ['–ü–æ–∏—Å–∫', 'Search', '–ö–∞—Ç–∞–ª–æ–≥', 'Catalog']):

                    return None
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å—Å—ã–ª–∫–∏ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è (acquisition) - —ç—Ç–æ –æ—Ç–ª–∏—á–∞–µ—Ç –∫–Ω–∏–≥–∏ –æ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            has_download_links = False
            for link in entry.findall('atom:link', ns):
                rel = link.get('rel')
                if rel in ['http://opds-spec.org/acquisition', 'http://opds-spec.org/acquisition/open-access']:
                    has_download_links = True
                    break
            
            # –ï—Å–ª–∏ –Ω–µ—Ç —Å—Å—ã–ª–æ–∫ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è, —ç—Ç–æ —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è, –∞ –Ω–µ –∫–Ω–∏–≥–∞
            if not has_download_links:
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏/–∂–∞–Ω—Ä—ã
            categories = []
            for category in entry.findall('atom:category', ns):
                term = category.get('term')
                label = category.get('label')
                if term:
                    categories.append(term)
                elif label:
                    categories.append(label)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ —Å—Ç—Ä–æ–∫—É –∂–∞–Ω—Ä–∞
            genre = ', '.join(categories) if categories else ''
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ –æ–±–ª–æ–∂–∫–∏
            download_links = []
            cover_url = None
            
            for link in entry.findall('atom:link', ns):
                rel = link.get('rel')
                href = link.get('href')
                link_type = link.get('type', '')
                
                # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è (acquisition)
                if rel in ['http://opds-spec.org/acquisition', 'http://opds-spec.org/acquisition/open-access']:
                    format_name = self._extract_format_from_type(link_type)
                    
                    if href and format_name:
                        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                        if href.startswith('/'):
                            full_url = f"{self.base_url}{href}"
                        else:
                            full_url = href
                            
                        download_links.append({
                            'format': format_name,
                            'url': full_url,
                            'type': link_type
                        })
                
                # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ–±–ª–æ–∂–∫–∏
                elif rel in ['http://opds-spec.org/image', 'http://opds-spec.org/image/thumbnail']:
                    if href and link_type.startswith('image/'):
                        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL –¥–ª—è –æ–±–ª–æ–∂–∫–∏
                        if href.startswith('/'):
                            cover_url = f"{self.base_url}{href}"
                        else:
                            cover_url = href
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∫–Ω–∏–≥–∏ –∏–∑ —Å—Å—ã–ª–æ–∫
            book_id = self._extract_book_id_from_links(download_links)
            
            # –ï—Å–ª–∏ –æ–±–ª–æ–∂–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ OPDS, –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–∑ –≤–Ω–µ—à–Ω–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            if not cover_url:
                title_text = title.text.strip() if title is not None else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'
                author_text = author.text.strip() if author is not None else None
                
                # –ü–æ–ª—É—á–∞–µ–º –æ–±–ª–æ–∂–∫—É, –ø–µ—Ä–µ–¥–∞–≤–∞—è ID –∫–Ω–∏–≥–∏ –¥–ª—è flibusta.su
                cover_url = get_book_cover_url(title_text, author_text, book_id)
                self.logger.info(f"–ü–æ–ª—É—á–µ–Ω–∞ –æ–±–ª–æ–∂–∫–∞ –¥–ª—è '{title_text}' (ID: {book_id}): {cover_url}")
            
            book_data = {
                'title': title.text.strip() if title is not None else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è',
                'author': author.text.strip() if author is not None else '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä',
                'description': summary.text.strip() if summary is not None else '',
                'updated': updated.text if updated is not None else '',
                'download_links': download_links,
                'source': 'flibusta',
                'source_id': book_id,
                'genre': genre,
                'cover_url': cover_url
            }
            
            print(f"  ‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–∏–ª–∏ –∫–Ω–∏–≥—É: '{book_data['title']}' - {book_data['author']} (ID: {book_id})")
            return book_data
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–∞–ø–∏—Å–∏ –∫–Ω–∏–≥–∏: {e}")
            return None
    

    
    def _extract_format_from_type(self, mime_type: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ —Ñ–∞–π–ª–∞ –∏–∑ MIME —Ç–∏–ø–∞"""
        format_mapping = {
            'application/fb2+xml': 'fb2',
            'application/epub+zip': 'epub',
            'application/x-mobipocket-ebook': 'mobi',
            'text/plain': 'txt',
            'application/pdf': 'pdf'
        }
        return format_mapping.get(mime_type, mime_type.split('/')[-1])
    
    def _extract_book_id_from_links(self, download_links: List[Dict]) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ID –∫–Ω–∏–≥–∏ –∏–∑ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ"""
        for link in download_links:
            url = link.get('url', '')
            # –ü—Ä–∏–º–µ—Ä: /b/12345/download -> 12345
            if '/b/' in url:
                parts = url.split('/b/')
                if len(parts) > 1:
                    book_id = parts[1].split('/')[0]
                    if book_id.isdigit():
                        return book_id
        return None
    
    def _parse_opds_entry(self, entry, namespaces: Dict[str, str]) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏ –∏–∑ OPDS"""
        try:
            title_elem = entry.find('atom:title', namespaces)
            title = title_elem.text if title_elem is not None else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ"
            
            author_elem = entry.find('atom:author/atom:name', namespaces)
            author = author_elem.text if author_elem is not None else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä"
            
            summary_elem = entry.find('atom:summary', namespaces)
            description = summary_elem.text if summary_elem is not None else ""
            
            # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
            download_links = {}
            links = entry.findall('atom:link', namespaces)
            
            for link in links:
                rel = link.get('rel')
                href = link.get('href')
                type_attr = link.get('type')
                
                if rel == 'http://opds-spec.org/acquisition':
                    if 'fb2' in type_attr:
                        download_links['fb2'] = urljoin(self.base_url, href)
                    elif 'epub' in type_attr:
                        download_links['epub'] = urljoin(self.base_url, href)
                    elif 'txt' in type_attr:
                        download_links['txt'] = urljoin(self.base_url, href)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∫–Ω–∏–≥–∏ –∏–∑ —Å—Å—ã–ª–æ–∫ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ–±–ª–æ–∂–∫–∏
            book_id = self._extract_book_id_from_links([{'url': link['url']} for link in download_links.values()])
            
            # –ü–æ–ª—É—á–∞–µ–º –æ–±–ª–æ–∂–∫—É –∏–∑ –≤–Ω–µ—à–Ω–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –ø–µ—Ä–µ–¥–∞–≤–∞—è ID –∫–Ω–∏–≥–∏
            cover_url = get_book_cover_url(title, author, book_id)
            
            return {
                'title': title,
                'author': author,
                'description': description,
                'download_links': download_links,
                'source': 'flibusta',
                'source_id': book_id,
                'cover_url': cover_url
            }
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–∞–ø–∏—Å–∏ OPDS: {e}")
            return None
    
    def download_book(self, book_data: Dict[str, Any], format_preference: str = 'fb2') -> Optional[str]:
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏ —Å –§–ª–∏–±—É—Å—Ç—ã"""
        try:
            title = book_data.get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–Ω–∏–≥–∞')
            author = book_data.get('author', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä')
            print(f"üì• –ù–∞—á–∏–Ω–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏: '{title}' - {author}")
            
            download_links = book_data.get('download_links', {})
            
            if not download_links:
                # –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä—è–º—ã—Ö —Å—Å—ã–ª–æ–∫, –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏—Ö —á–µ—Ä–µ–∑ ID –∫–Ω–∏–≥–∏
                source_id = book_data.get('source_id')
                if source_id:
                    download_links = self._get_download_links_by_id(source_id)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ª–æ–≤–∞—Ä—å —Å—Å—ã–ª–æ–∫ –≤ —Å–ø–∏—Å–æ–∫, –µ—Å–ª–∏ —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å
            if isinstance(download_links, dict):
                links_list = []
                for format_key, link_data in download_links.items():
                    if isinstance(link_data, dict):
                        link_data['format'] = format_key
                        links_list.append(link_data)
                download_links = links_list
            
            # –ò—â–µ–º –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç (–≤–∫–ª—é—á–∞—è –∞—Ä—Ö–∏–≤—ã)
            preferred_link = None
            for link in download_links:
                link_format = link.get('format', '')
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–ª–∏ —Ñ–æ—Ä–º–∞—Ç+zip
                if (link_format == format_preference or 
                    link_format == f"{format_preference}+zip" or
                    link_format.startswith(f"{format_preference}+")):
                    preferred_link = link
                    break
            
            # –ï—Å–ª–∏ –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—â–µ–º —Ç–æ–ª—å–∫–æ FB2 –∏–ª–∏ EPUB (–≤–∫–ª—é—á–∞—è –∞—Ä—Ö–∏–≤—ã)
            if not preferred_link and download_links:
                # –°—Ç—Ä–æ–≥–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: fb2 (–≤–∫–ª—é—á–∞—è fb2+zip) > epub
                format_priority = ['fb2', 'epub']
                for fmt in format_priority:
                    for link in download_links:
                        link_format = link.get('format', '')
                        if (link_format == fmt or 
                            link_format == f"{fmt}+zip" or
                            link_format.startswith(f"{fmt}+")):
                            preferred_link = link
                            break
                    if preferred_link:
                        break
                
                # –ï—Å–ª–∏ –Ω–µ—Ç FB2 –∏–ª–∏ EPUB, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–Ω–∏–≥—É
                if not preferred_link:
                    self.logger.info(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–Ω–∏–≥—É '{book_data.get('title')}' - –Ω–µ—Ç FB2 –∏–ª–∏ EPUB —Ñ–æ—Ä–º–∞—Ç–æ–≤")
                    return None
            
            if not preferred_link:
                self.logger.error(f"–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∫–Ω–∏–≥–∏: {book_data.get('title')}")
                return None
            
            # –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª
            download_url = preferred_link['url']
            if not download_url.startswith('http'):
                download_url = urljoin(self.base_url, download_url)
            
            format_name = preferred_link.get('format', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π')
            print(f"üîó –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª –≤ —Ñ–æ—Ä–º–∞—Ç–µ {format_name} —Å URL: {download_url}")
            self.logger.info(f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏: {book_data.get('title')} –≤ —Ñ–æ—Ä–º–∞—Ç–µ {format_name}")
            
            response = self.session.get(download_url, timeout=self.timeout * 2, stream=True)
            response.raise_for_status()
            print(f"‚úÖ HTTP –∑–∞–ø—Ä–æ—Å —É—Å–ø–µ—à–µ–Ω, —Å—Ç–∞—Ç—É—Å: {response.status_code}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
            content_length = response.headers.get('content-length')
            if content_length and int(content_length) > EXTERNAL_SOURCES_CONFIG['max_file_size']:
                self.logger.error(f"–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π: {content_length} –±–∞–π—Ç")
                return None
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
            print(f"üì¶ –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞...")
            content = b''
            total_size = 0
            for chunk in response.iter_content(chunk_size=8192):
                content += chunk
                total_size += len(chunk)
                if len(content) > EXTERNAL_SOURCES_CONFIG['max_file_size']:
                    self.logger.error("–ü—Ä–µ–≤—ã—à–µ–Ω –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞")
                    return None
            
            print(f"‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω, —Ä–∞–∑–º–µ—Ä: {total_size} –±–∞–π—Ç")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É –∏ –¥–µ–∫–æ–¥–∏—Ä—É–µ–º
            print(f"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ {format_name}...")
            result = self._decode_content(content, preferred_link.get('format', 'fb2'))
            
            if result:
                print(f"‚úÖ –ö–Ω–∏–≥–∞ '{title}' —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–Ω–∏–≥–∏ '{title}'")
                
            return result
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Å –§–ª–∏–±—É—Å—Ç—ã: {e}")
            return None
    
    def _get_download_links_by_id(self, book_id: str) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –ø–æ ID –∫–Ω–∏–≥–∏"""
        try:
            book_url = f"{self.base_url}/b/{book_id}"
            response = self.session.get(book_url, timeout=self.timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            download_links = []
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤ HTML
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                if '/b/' in href and '/download' in href:
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç –ø–æ —Ç–µ–∫—Å—Ç—É —Å—Å—ã–ª–∫–∏ –∏–ª–∏ URL
                    link_text = link.text.lower()
                    format_name = 'fb2'  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    
                    for fmt in FLIBUSTA_CONFIG['supported_formats']:
                        if fmt in link_text or fmt in href:
                            format_name = fmt
                            break
                    
                    download_links.append({
                        'format': format_name,
                        'url': urljoin(self.base_url, href)
                    })
            
            return download_links
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫ –¥–ª—è –∫–Ω–∏–≥–∏ {book_id}: {e}")
            return []
    
    def get_book_description(self, book_id: str) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∫–Ω–∏–≥–∏ –ø–æ ID"""
        try:
            book_url = f"{self.base_url}/b/{book_id}"
            response = self.session.get(book_url, timeout=self.timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò—â–µ–º —Ä–∞–∑–¥–µ–ª —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏–µ–π
            annotation_header = soup.find('h2', string='–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è')
            if annotation_header:
                # –ù–∞—Ö–æ–¥–∏–º —Å–ª–µ–¥—É—é—â–∏–π —ç–ª–µ–º–µ–Ω—Ç –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ "–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è"
                annotation_content = []
                current_element = annotation_header.next_sibling
                
                while current_element:
                    if hasattr(current_element, 'name'):
                        # –ï—Å–ª–∏ —ç—Ç–æ —Ç–µ–≥
                        if current_element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                            # –ï—Å–ª–∏ –≤—Å—Ç—Ä–µ—Ç–∏–ª–∏ –¥—Ä—É–≥–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º
                            break
                        elif current_element.name == 'p':
                            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
                            text = current_element.get_text(strip=True)
                            if text and text != '&nbsp;':
                                annotation_content.append(text)
                        elif current_element.name == 'br':
                            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
                            pass
                    else:
                        # –ï—Å–ª–∏ —ç—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —É–∑–µ–ª
                        text = str(current_element).strip()
                        if text and text != '&nbsp;':
                            annotation_content.append(text)
                    
                    current_element = current_element.next_sibling
                
                if annotation_content:
                    return '\n\n'.join(annotation_content)
            
            self.logger.info(f"–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è –¥–ª—è –∫–Ω–∏–≥–∏ {book_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return None
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è –∫–Ω–∏–≥–∏ {book_id}: {e}")
            return None
    
    def _decode_content(self, content: bytes, file_format: str) -> str:
        """–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞"""
        try:
            print(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞: {file_format}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª ZIP –∞—Ä—Ö–∏–≤–æ–º
            if self._is_zip_archive(content):
                print(f"üì¶ –û–±–Ω–∞—Ä—É–∂–µ–Ω ZIP –∞—Ä—Ö–∏–≤, –Ω–∞—á–∏–Ω–∞–µ–º —Ä–∞–∑–∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏–µ...")
                return self._extract_from_zip(content, file_format)
            else:
                print(f"üìÑ –û–±—ã—á–Ω—ã–π —Ñ–∞–π–ª, –Ω–∞—á–∏–Ω–∞–µ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ...")
            
            # –î–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –ø—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–¥–∏—Ä–æ–≤–∫—É
            if file_format in ['fb2', 'txt']:
                if EXTERNAL_SOURCES_CONFIG['encoding_detection']:
                    detected = chardet.detect(content)
                    encoding = detected.get('encoding', 'utf-8')
                    confidence = detected.get('confidence', 0)
                    
                    if confidence > 0.7:
                        try:
                            return content.decode(encoding)
                        except UnicodeDecodeError:
                            pass
                
                # –ü—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
                for encoding in ['utf-8', 'windows-1251', 'cp1251', 'koi8-r']:
                    try:
                        return content.decode(encoding)
                    except UnicodeDecodeError:
                        continue
                
                # –í –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏
                return content.decode('utf-8', errors='ignore')
            
            elif file_format == 'epub':
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ EPUB —Ñ–∞–π–ª–æ–≤
                return self._parse_epub(content)
            
            else:
                # –î–ª—è –±–∏–Ω–∞—Ä–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å (base64 –∏–ª–∏ hex)
                import base64
                return base64.b64encode(content).decode('ascii')
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ: {e}")
            return content.decode('utf-8', errors='ignore')
    
    def _is_zip_archive(self, content: bytes) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª ZIP –∞—Ä—Ö–∏–≤–æ–º"""
        try:
            return zipfile.is_zipfile(io.BytesIO(content))
        except Exception:
            return False
    
    def _extract_from_zip(self, content: bytes, expected_format: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∏–∑ ZIP –∞—Ä—Ö–∏–≤–∞"""
        try:
            with zipfile.ZipFile(io.BytesIO(content), 'r') as zip_file:
                # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –≤ –∞—Ä—Ö–∏–≤–µ
                file_list = zip_file.namelist()
                print(f"üìã –§–∞–π–ª—ã –≤ –∞—Ä—Ö–∏–≤–µ: {file_list}")
                
                # –ò—â–µ–º —Ñ–∞–π–ª –Ω—É–∂–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
                target_file = None
                format_extensions = {
                    'fb2': ['.fb2'],
                    'epub': ['.epub'],
                    'txt': ['.txt'],
                    'pdf': ['.pdf'],
                    'mobi': ['.mobi']
                }
                
                # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º —Ñ–∞–π–ª —Å –Ω—É–∂–Ω—ã–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º
                if expected_format in format_extensions:
                    for ext in format_extensions[expected_format]:
                        for filename in file_list:
                            if filename.lower().endswith(ext):
                                target_file = filename
                                break
                        if target_file:
                            break
                
                # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Ñ–∞–π–ª
                if not target_file:
                    for filename in file_list:
                        if not filename.endswith('/'):
                            for ext_list in format_extensions.values():
                                if any(filename.lower().endswith(ext) for ext in ext_list):
                                    target_file = filename
                                    break
                            if target_file:
                                break
                
                # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ –Ω–∞–π–¥–µ–Ω, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —Ñ–∞–π–ª
                if not target_file and file_list:
                    target_file = file_list[0]
                
                if target_file:
                    print(f"üéØ –ù–∞–π–¥–µ–Ω —Ü–µ–ª–µ–≤–æ–π —Ñ–∞–π–ª: {target_file}")
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∞–π–ª
                    print(f"üì§ –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∞–π–ª –∏–∑ –∞—Ä—Ö–∏–≤–∞...")
                    extracted_content = zip_file.read(target_file)
                    print(f"‚úÖ –§–∞–π–ª –∏–∑–≤–ª–µ—á–µ–Ω, —Ä–∞–∑–º–µ—Ä: {len(extracted_content)} –±–∞–π—Ç")
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é —Ñ–∞–π–ª–∞
                    file_format = expected_format
                    for fmt, extensions in format_extensions.items():
                        if any(target_file.lower().endswith(ext) for ext in extensions):
                            file_format = fmt
                            break
                    
                    print(f"üìù –û–ø—Ä–µ–¥–µ–ª–µ–Ω —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {file_format}")
                    self.logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω —Ñ–∞–π–ª {target_file} –∏–∑ –∞—Ä—Ö–∏–≤–∞, —Ñ–æ—Ä–º–∞—Ç: {file_format}")
                    
                    # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                    print(f"üîÑ –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ñ–∞–π–ª...")
                    return self._decode_content(extracted_content, file_format)
                else:
                    print(f"‚ùå –í –∞—Ä—Ö–∏–≤–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ñ–∞–π–ª–æ–≤")
                    self.logger.error("–í –∞—Ä—Ö–∏–≤–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ñ–∞–π–ª–æ–≤")
                    return "–û—à–∏–±–∫–∞: –≤ –∞—Ä—Ö–∏–≤–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ñ–∞–π–ª–æ–≤"
                    
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑ ZIP –∞—Ä—Ö–∏–≤–∞: {e}")
            return f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑ –∞—Ä—Ö–∏–≤–∞: {e}"
    
    def _parse_epub(self, content: bytes) -> str:
        """–ü–∞—Ä—Å–∏–Ω–≥ EPUB —Ñ–∞–π–ª–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞"""
        try:
            with zipfile.ZipFile(io.BytesIO(content), 'r') as epub_zip:
                # –ò—â–µ–º —Ñ–∞–π–ª—ã —Å —Ç–µ–∫—Å—Ç–æ–º (–æ–±—ã—á–Ω–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ XHTML)
                text_parts = []
                
                for filename in epub_zip.namelist():
                    if filename.endswith(('.xhtml', '.html', '.htm')):
                        try:
                            file_content = epub_zip.read(filename)
                            # –ü–∞—Ä—Å–∏–º HTML/XHTML —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                            soup = BeautifulSoup(file_content, 'html.parser')
                            
                            # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
                            for script in soup(["script", "style"]):
                                script.decompose()
                            
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
                            text = soup.get_text()
                            
                            # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
                            lines = (line.strip() for line in text.splitlines())
                            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                            text = '\n'.join(chunk for chunk in chunks if chunk)
                            
                            if text.strip():
                                text_parts.append(text)
                                
                        except Exception as e:
                            self.logger.warning(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {filename} –≤ EPUB: {e}")
                            continue
                
                if text_parts:
                    return '\n\n'.join(text_parts)
                else:
                    return "–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ EPUB —Ñ–∞–π–ª–∞"
                    
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ EPUB: {e}")
            return f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ EPUB: {e}"


# LibGenClient —É–¥–∞–ª–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ Flibusta


class ExternalBookSources:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–Ω–µ—à–Ω–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –∫–Ω–∏–≥ (—Ç–æ–ª—å–∫–æ Flibusta)"""
    
    def __init__(self, use_tor_for_flibusta: bool = True):
        self.flibusta = FlibustaTorClient(use_tor=use_tor_for_flibusta)
    
    def search_flibusta(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –≤–æ Flibusta"""
        try:
            return self.flibusta.search_books(query, limit)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –§–ª–∏–±—É—Å—Ç–µ: {e}")
            return []
    
    def get_book_description(self, book_id: str) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∫–Ω–∏–≥–∏ –∏–∑ Flibusta"""
        try:
            return self.flibusta.get_book_description(book_id)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∫–Ω–∏–≥–∏: {e}")
            return None
    
    def get_book_content(self, book_data: Dict[str, Any], format_type: str = 'fb2') -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∫–Ω–∏–≥–∏ –∏–∑ –≤–Ω–µ—à–Ω–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –∏–º–µ—é—â–∏–µ—Å—è –¥–∞–Ω–Ω—ã–µ –∫–Ω–∏–≥–∏
            content = self.flibusta.download_book(book_data, format_type)
            if content:
                # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∏–∑ FB2/EPUB –≤ —Ç–µ–∫—Å—Ç
                return self._convert_to_text(content, format_type)
            
            return None
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∫–Ω–∏–≥–∏: {e}")
            return None
    
    def search_books_by_category(self, category: str, limit: int = 10) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –∫–Ω–∏–≥ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤–æ –§–ª–∏–±—É—Å—Ç–µ"""
        try:
            # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω URL, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –Ω–∞–ø—Ä—è–º—É—é
            if category.startswith('http'):
                return self.flibusta.browse_books_by_category(category, sort_by_popularity=True, limit=limit)
            
            # –ò–Ω–∞—á–µ –∏—â–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
            categories = self.flibusta.browse_categories()
            
            # –ò—â–µ–º –Ω—É–∂–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            category_url = None
            for cat in categories:
                if category.lower() in cat.get('name', '').lower():
                    category_url = cat.get('url')
                    break
            
            if not category_url:
                # –ï—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –¥–µ–ª–∞–µ–º –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫
                return self.search_flibusta(category, limit)
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–Ω–∏–≥–∏ –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            return self.flibusta.browse_books_by_category(category_url, sort_by_popularity=True, limit=limit)
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {e}")
            return []
    
    def get_book_cover_url(self, book_id: str, title: str, author: str) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ URL –æ–±–ª–æ–∂–∫–∏ –∫–Ω–∏–≥–∏"""
        try:
            return get_book_cover_url(title, author)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –æ–±–ª–æ–∂–∫–∏: {e}")
            return None
    
    def _convert_to_text(self, content: str, format_type: str) -> str:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∫–Ω–∏–≥–∏ –≤ —Ç–µ–∫—Å—Ç"""
        try:
            if format_type == 'txt':
                return content
            elif format_type == 'fb2':
                # –ü–∞—Ä—Å–∏–Ω–≥ FB2 XML
                return self._parse_fb2(content.encode('utf-8'))
            elif format_type == 'epub':
                # –î–ª—è EPUB —Ñ–∞–π–ª–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç —É–∂–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω –≤ _decode_content
                # –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞ base64, –¥–µ–∫–æ–¥–∏—Ä—É–µ–º –∏ –ø–∞—Ä—Å–∏–º
                try:
                    import base64
                    epub_bytes = base64.b64decode(content)
                    return self._parse_epub(epub_bytes)
                except Exception:
                    # –ï—Å–ª–∏ –Ω–µ base64, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
                    return content
            else:
                return content
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {e}")
            return ""
    
    def _parse_fb2(self, fb2_content: bytes) -> str:
        """–ü–∞—Ä—Å–∏–Ω–≥ FB2 —Ñ–∞–π–ª–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞"""
        try:
            root = ET.fromstring(fb2_content)
            
            # FB2 namespace
            ns = {'fb': 'http://www.gribuser.ru/xml/fictionbook/2.0'}
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Å–µ–∫—Ü–∏–π
            text_parts = []
            
            # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
            paragraphs = root.findall('.//fb:p', ns)
            for p in paragraphs:
                if p.text:
                    text_parts.append(p.text.strip())
            
            return '\n\n'.join(text_parts)
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ FB2: {e}")
            return ""


# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Django
def search_external_books(query: str, sources: List[str] = None, use_tor: bool = True, limit: int = 10) -> Dict[str, List[Dict[str, Any]]]:
    """–ü–æ–∏—Å–∫ –∫–Ω–∏–≥ –≤–æ –≤–Ω–µ—à–Ω–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö (—Ç–æ–ª—å–∫–æ Flibusta)
    
    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        sources: –°–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
        use_tor: –ï—Å–ª–∏ True - –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ onion-–∞–¥—Ä–µ—Å—É —á–µ—Ä–µ–∑ VPN, –µ—Å–ª–∏ False - –ø—Ä—è–º–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ clearnet –∑–µ—Ä–∫–∞–ª–∞–º
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    if sources is None:
        sources = ['flibusta']
    
    # use_tor=True: –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ onion-–∞–¥—Ä–µ—Å—É —á–µ—Ä–µ–∑ VPN
    # use_tor=False: –ø—Ä—è–º–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ clearnet –∑–µ—Ä–∫–∞–ª–∞–º
    external_sources = ExternalBookSources(use_tor_for_flibusta=use_tor)
    
    results = {'flibusta': []}
    
    # –ü–æ–∏—Å–∫ —Ç–æ–ª—å–∫–æ –≤ Flibusta
    if 'flibusta' in sources:
        flibusta_results = external_sources.search_flibusta(query, limit)
        results['flibusta'] = flibusta_results
    
    return results


def import_book_from_external_source(book_data: Dict[str, Any], source: str, download_format: str = 'fb2', use_tor: bool = True) -> Optional[str]:
    """–ò–º–ø–æ—Ä—Ç –∫–Ω–∏–≥–∏ –∏–∑ –≤–Ω–µ—à–Ω–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (—Ç–æ–ª—å–∫–æ –§–ª–∏–±—É—Å—Ç–∞)
    
    Args:
        book_data: –î–∞–Ω–Ω—ã–µ –∫–Ω–∏–≥–∏
        source: –ò—Å—Ç–æ—á–Ω–∏–∫ –∫–Ω–∏–≥–∏ (—Ç–æ–ª—å–∫–æ flibusta)
        download_format: –§–æ—Ä–º–∞—Ç –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è (fb2, epub)
        use_tor: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Tor –¥–ª—è onion-–∞–¥—Ä–µ—Å–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
    """
    if source != 'flibusta':
        raise ValueError("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –∏—Å—Ç–æ—á–Ω–∏–∫ 'flibusta'")
    
    sources = ExternalBookSources(use_tor_for_flibusta=use_tor)
    return sources.get_book_content(book_data, download_format)