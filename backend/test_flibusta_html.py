#!/usr/bin/env python
"""
Ð¢ÐµÑÑ‚ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° HTML-ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹ Flibusta
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time

def test_flibusta_html():
    """Ð¢ÐµÑÑ‚ HTML-ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹ Flibusta"""
    
    print("=== ÐÐ½Ð°Ð»Ð¸Ð· HTML-ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹ Flibusta ===")
    
    # Ð‘Ð°Ð·Ð¾Ð²Ñ‹Ðµ URL Ð´Ð»Ñ Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
    base_urls = [
        'http://flibusta.is',
        'http://flibusta.site',
        'http://flibustahezeous3.onion'  # Tor Ð°Ð´Ñ€ÐµÑ
    ]
    
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    })
    
    for base_url in base_urls:
        print(f"\nðŸ” Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ: {base_url}")
        
        try:
            # Ð¢ÐµÑÑ‚ Ð³Ð»Ð°Ð²Ð½Ð¾Ð¹ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹
            print("   ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð³Ð»Ð°Ð²Ð½Ð¾Ð¹ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹...")
            response = session.get(base_url, timeout=10)
            if response.status_code == 200:
                print(f"   âœ… Ð“Ð»Ð°Ð²Ð½Ð°Ñ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð° Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð° (ÑÑ‚Ð°Ñ‚ÑƒÑ: {response.status_code})")
                
                # Ð¢ÐµÑÑ‚ Ð¿Ð¾Ð¸ÑÐºÐ°
                search_url = f"{base_url}/booksearch?ask=ÐŸÑƒÑˆÐºÐ¸Ð½"
                print(f"   ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¿Ð¾Ð¸ÑÐºÐ°: {search_url}")
                
                search_response = session.get(search_url, timeout=10)
                if search_response.status_code == 200:
                    print(f"   âœ… ÐŸÐ¾Ð¸ÑÐº Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ (ÑÑ‚Ð°Ñ‚ÑƒÑ: {search_response.status_code})")
                    
                    # ÐÐ½Ð°Ð»Ð¸Ð· HTML ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹
                    soup = BeautifulSoup(search_response.content, 'html.parser')
                    
                    print(f"   ðŸ“„ ÐÐ½Ð°Ð»Ð¸Ð· HTML:")
                    print(f"      Title: {soup.title.text if soup.title else 'ÐÐµÑ‚ title'}")
                    
                    # Ð˜Ñ‰ÐµÐ¼ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ðµ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ñ‹Ðµ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹
                    structures_to_check = [
                        ('div.book-item', 'Ð‘Ð»Ð¾ÐºÐ¸ ÐºÐ½Ð¸Ð³'),
                        ('a[href*="/b/"]', 'Ð¡ÑÑ‹Ð»ÐºÐ¸ Ð½Ð° ÐºÐ½Ð¸Ð³Ð¸'),
                        ('table tr', 'Ð¡Ñ‚Ñ€Ð¾ÐºÐ¸ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹'),
                        ('ul li', 'Ð­Ð»ÐµÐ¼ÐµÐ½Ñ‚Ñ‹ ÑÐ¿Ð¸ÑÐºÐ°'),
                        ('div', 'Ð’ÑÐµ div ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ñ‹'),
                        ('a', 'Ð’ÑÐµ ÑÑÑ‹Ð»ÐºÐ¸')
                    ]
                    
                    for selector, description in structures_to_check:
                        elements = soup.select(selector)
                        print(f"      {description}: {len(elements)} ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð²")
                        
                        if len(elements) > 0 and len(elements) < 20:  # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ Ð½Ðµ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð¼Ð½Ð¾Ð³Ð¾
                            for i, elem in enumerate(elements[:5]):
                                text = elem.get_text(strip=True)[:100]
                                href = elem.get('href', '')
                                print(f"        {i+1}. {text} {href}")
                    
                    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ HTML Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
                    with open(f'/tmp/flibusta_search_{base_url.replace("://", "_").replace(".", "_")}.html', 'w', encoding='utf-8') as f:
                        f.write(search_response.text)
                    print(f"   ðŸ’¾ HTML ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½ Ð² /tmp/")
                    
                else:
                    print(f"   âŒ ÐŸÐ¾Ð¸ÑÐº Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½ (ÑÑ‚Ð°Ñ‚ÑƒÑ: {search_response.status_code})")
                    
            else:
                print(f"   âŒ Ð“Ð»Ð°Ð²Ð½Ð°Ñ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð° Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð° (ÑÑ‚Ð°Ñ‚ÑƒÑ: {response.status_code})")
                
        except requests.exceptions.Timeout:
            print(f"   â° Ð¢Ð°Ð¹Ð¼Ð°ÑƒÑ‚ Ð¿Ñ€Ð¸ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ð¸ Ðº {base_url}")
        except requests.exceptions.ConnectionError:
            print(f"   ðŸ”Œ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ Ðº {base_url}")
        except Exception as e:
            print(f"   âŒ ÐžÑˆÐ¸Ð±ÐºÐ°: {e}")
        
        time.sleep(1)  # ÐŸÐ°ÑƒÐ·Ð° Ð¼ÐµÐ¶Ð´Ñƒ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°Ð¼Ð¸

def test_opds_structure():
    """Ð¢ÐµÑÑ‚ OPDS ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹"""
    
    print("\n=== ÐÐ½Ð°Ð»Ð¸Ð· OPDS ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹ ===")
    
    opds_urls = [
        'http://flibusta.is/opds/search?searchTerm=ÐŸÑƒÑˆÐºÐ¸Ð½',
        'http://flibusta.site/opds/search?searchTerm=ÐŸÑƒÑˆÐºÐ¸Ð½'
    ]
    
    session = requests.Session()
    
    for opds_url in opds_urls:
        print(f"\nðŸ” Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ OPDS: {opds_url}")
        
        try:
            response = session.get(opds_url, timeout=10)
            if response.status_code == 200:
                print(f"   âœ… OPDS Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½ (ÑÑ‚Ð°Ñ‚ÑƒÑ: {response.status_code})")
                print(f"   Content-Type: {response.headers.get('content-type', 'unknown')}")
                
                # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ XML Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
                with open(f'/tmp/flibusta_opds_{opds_url.split("//")[1].split("/")[0]}.xml', 'w', encoding='utf-8') as f:
                    f.write(response.text)
                print(f"   ðŸ’¾ OPDS XML ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½ Ð² /tmp/")
                
                # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ðµ 500 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²
                print(f"   ðŸ“„ ÐŸÐµÑ€Ð²Ñ‹Ðµ 500 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²:")
                print(f"   {response.text[:500]}...")
                
            else:
                print(f"   âŒ OPDS Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½ (ÑÑ‚Ð°Ñ‚ÑƒÑ: {response.status_code})")
                
        except Exception as e:
            print(f"   âŒ ÐžÑˆÐ¸Ð±ÐºÐ° OPDS: {e}")
        
        time.sleep(1)

if __name__ == '__main__':
    print("ðŸš€ Ð—Ð°Ð¿ÑƒÑÐº Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Flibusta\n")
    
    test_flibusta_html()
    test_opds_structure()
    
    print("\nâœ… ÐÐ½Ð°Ð»Ð¸Ð· Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½!")
    print("\nðŸ“ ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð½Ñ‹Ðµ Ñ„Ð°Ð¹Ð»Ñ‹ Ð² /tmp/ Ð´Ð»Ñ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°")